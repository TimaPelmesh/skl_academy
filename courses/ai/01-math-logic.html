<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Модуль 1: Математика и логика для Data Scientist — линейная алгебра, производные, градиент, вероятность, статистика">
    <title>1. Фундамент: математика и логика | Введение в AI | SKL Academy</title>
    <link rel="canonical" href="https://skl-academy.ru/courses/ai/01-math-logic.html">
    <link rel="stylesheet" href="../courses.css">
    <link rel="icon" href="../../images/icon.ico" type="image/x-icon">
</head>
<body data-course="ai">
    <header>
        <div class="menu-toggle" id="menuToggle"><span></span><span></span><span></span></div>
        <h1 class="header-title">Введение в AI</h1>
        <div class="header-buttons">
            <div class="color-scheme-buttons"><button class="color-scheme-btn" data-color="purple"><span class="color-dot" style="background: #6c63ff;"></span></button></div>
            <button class="theme-toggle-btn" id="themeToggle" title="Переключить тему" aria-label="Тема"><svg class="theme-icon-sun" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path d="M12 2.25a.75.75 0 01.75.75v2.25a.75.75 0 01-1.5 0V3a.75.75 0 01.75-.75zM7.5 12a4.5 4.5 0 119 0 4.5 4.5 0 01-9 0z"/></svg><svg class="theme-icon-moon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path fill-rule="evenodd" d="M9.528 1.718a.75.75 0 01.162.819A8.97 8.97 0 009 6a9 9 0 009 9 8.97 8.97 0 003.463-.69.75.75 0 01.981.98 10.503 10.503 0 01-9.694 6.46c-5.799 0-10.5-4.701-10.5-10.5 0-4.368 2.667-8.112 6.46-9.694a.75.75 0 01.818.162z" clip-rule="evenodd"/></svg></button>
            <button class="mobile-settings-btn" id="mobileSettingsTrigger" aria-label="Настройки"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M12.22 2h-.44a2 2 0 0 0-2 2v.18a2 2 0 0 1-1 1.73l-.43.25a2 2 0 0 1-2 0l-.15-.08a2 2 0 0 0-2.73.73l-.22.38a2 2 0 0 0 .73 2.73l.15.1a2 2 0 0 1 1 1.72v.51a2 2 0 0 1-1 1.74l-.15.09a2 2 0 0 0-.73 2.73l.22.38a2 2 0 0 0 2.73.73l.15-.08a2 2 0 0 1 2 0l.43.25a2 2 0 0 1 1 1.73V20a2 2 0 0 0 2 2h.44a2 2 0 0 0 2-2v-.18a2 2 0 0 1 1-1.73l.43-.25a2 2 0 0 1 2 0l.15.08a2 2 0 0 0 2.73-.73l.22-.39a2 2 0 0 0-.73-2.73l-.15-.08a2 2 0 0 1-1-1.74v-.5a2 2 0 0 1 1-1.74l.15-.09a2 2 0 0 0 .73-2.73l-.22-.38a2 2 0 0 0-2.73-.73l-.15.08a2 2 0 0 1-2 0l-.43-.25a2 2 0 0 1-1-1.73V4a2 2 0 0 0-2-2z"/><circle cx="12" cy="12" r="3"/></svg></button>
            <a href="../../index.html" class="exit-btn" title="На главную"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path d="M17 7l-1.41 1.41L18.17 11H8v2h10.17l-2.58 2.58L17 17l5-5zM4 5h8V3H4c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h8v-2H4V5z"/></svg></a>
        </div>
    </header>
    <div class="container">
        <div id="sidebar" class="sidebar">
            <div class="course-structure">
                <div class="topic">
                    <button class="topic-btn active">1. Фундамент: математика и логика</button>
                    <div class="subtopics" style="max-height: 1000px;">
                        <a href="#linear-algebra" class="subtopic active">1.1 Линейная алгебра для данных</a>
                        <a href="#derivatives-gradient" class="subtopic">1.2 Производные и градиент</a>
                        <a href="#probability" class="subtopic">1.3 Теория вероятностей</a>
                        <a href="#statistics" class="subtopic">1.4 Основы статистики</a>
                    </div>
                </div>
                <div class="topic">
                    <button class="topic-btn">2. Введение в ML и типы задач</button>
                    <div class="subtopics">
                        <a href="02-ml-intro.html#supervised" class="subtopic">2.1 Обучение с учителем</a>
                        <a href="02-ml-intro.html#unsupervised" class="subtopic">2.2 Обучение без учителя</a>
                        <a href="02-ml-intro.html#rl" class="subtopic">2.3 Обучение с подкреплением</a>
                        <a href="02-ml-intro.html#pipeline" class="subtopic">2.4 Пайплайн ML</a>
                    </div>
                </div>
                <div class="topic">
                    <button class="topic-btn">3. Классические алгоритмы</button>
                    <div class="subtopics">
                        <a href="03-classical-ml.html#linear-regression" class="subtopic">3.1 Линейная регрессия</a>
                        <a href="03-classical-ml.html#logistic" class="subtopic">3.2 Логистическая регрессия</a>
                        <a href="03-classical-ml.html#knn" class="subtopic">3.3 KNN</a>
                        <a href="03-classical-ml.html#decision-trees" class="subtopic">3.4 Решающие деревья</a>
                    </div>
                </div>
                <div class="topic">
                    <button class="topic-btn">4. Ансамблевые методы</button>
                    <div class="subtopics">
                        <a href="04-ensembles.html#random-forest" class="subtopic">4.1 Random Forest</a>
                        <a href="04-ensembles.html#gradient-boosting" class="subtopic">4.2 Градиентный бустинг</a>
                        <a href="04-ensembles.html#catboost" class="subtopic">4.3 CatBoost</a>
                        <a href="04-ensembles.html#lightgbm" class="subtopic">4.4 LightGBM</a>
                    </div>
                </div>
                <div class="topic">
                    <button class="topic-btn">5. Введение в нейросети</button>
                    <div class="subtopics">
                        <a href="05-neural-intro.html#perceptron" class="subtopic">5.1 Перцептрон</a>
                        <a href="05-neural-intro.html#activations" class="subtopic">5.2 Функции активации</a>
                        <a href="05-neural-intro.html#mlp" class="subtopic">5.3 MLP</a>
                        <a href="05-neural-intro.html#forward" class="subtopic">5.4 Forward Pass</a>
                        <a href="05-neural-intro.html#code-example" class="subtopic">5.5 Код на Python</a>
                        <a href="05-neural-intro.html#practice" class="subtopic">5.6 Практика</a>
                    </div>
                </div>
                <div class="topic">
                    <button class="topic-btn">6. Backpropagation и оптимизация</button>
                    <div class="subtopics">
                        <a href="06-backprop-optimization.html#loss" class="subtopic">6.1 Функция потерь</a>
                        <a href="06-backprop-optimization.html#backprop" class="subtopic">6.2 Backpropagation</a>
                        <a href="06-backprop-optimization.html#gradient-descent" class="subtopic">6.3 Градиентный спуск</a>
                        <a href="06-backprop-optimization.html#problems" class="subtopic">6.4 Проблемы обучения</a>
                    </div>
                </div>
                <div class="topic">
                    <button class="topic-btn">7. RNN и LSTM</button>
                    <div class="subtopics">
                        <a href="07-rnn-lstm.html#sequences" class="subtopic">7.1 Последовательные данные</a>
                        <a href="07-rnn-lstm.html#rnn" class="subtopic">7.2 RNN</a>
                        <a href="07-rnn-lstm.html#long-memory" class="subtopic">7.3 Долгосрочная память</a>
                        <a href="07-rnn-lstm.html#lstm" class="subtopic">7.4 LSTM</a>
                    </div>
                </div>
                <div class="topic">
                    <button class="topic-btn">8. Attention и Трансформеры</button>
                    <div class="subtopics">
                        <a href="08-attention-transformers.html#attention" class="subtopic">8.1 Механизм Attention</a>
                        <a href="08-attention-transformers.html#transformer" class="subtopic">8.2 Архитектура Transformer</a>
                        <a href="08-attention-transformers.html#positional" class="subtopic">8.3 Позиционное кодирование</a>
                        <a href="08-attention-transformers.html#multihead" class="subtopic">8.4 Multi-Head Attention</a>
                    </div>
                </div>
                <div class="topic">
                    <button class="topic-btn">9. Большие языковые модели (LLM)</button>
                    <div class="subtopics">
                        <a href="09-llm.html#tokenization" class="subtopic">9.1 Токенизация и эмбеддинги</a>
                        <a href="09-llm.html#pretraining" class="subtopic">9.2 Pre-training и Fine-tuning</a>
                        <a href="09-llm.html#temperature" class="subtopic">9.3 Температура и сэмплирование</a>
                        <a href="09-llm.html#context" class="subtopic">9.4 Окно контекста</a>
                    </div>
                </div>
                <div class="topic">
                    <button class="topic-btn">10. Практикум и MLOps</button>
                    <div class="subtopics">
                        <a href="10-practice-mlops.html#python-libs" class="subtopic">10.1 Библиотеки Python</a>
                        <a href="10-practice-mlops.html#sklearn" class="subtopic">10.2 Scikit-learn</a>
                        <a href="10-practice-mlops.html#pytorch-tf" class="subtopic">10.3 PyTorch / TensorFlow</a>
                        <a href="10-practice-mlops.html#mlops" class="subtopic">10.4 MLOps</a>
                    </div>
                </div>
            </div>
        </div>
        <main id="content" class="content sidebar-active">
            <section id="module-intro" class="topic-section">
                <h2>Модуль 1: Фундамент. Математика и логика для Data Scientist'а</h2>
                <div class="lesson-content">
                    <p>Перед тем как погружаться в нейросети, бустинг и большие языковые модели, важно заложить базу: без неё формулы в статьях и коде будут казаться «магией», а не инструментом. Этот модуль не заменяет полноценный курс по математике — он даёт <strong>минимальный набор идей</strong>, которые постоянно встречаются в ML и AI, и показывает, <strong>где именно</strong> они используются.</p>
                    <p><strong>Задача модуля:</strong> снять страх перед математикой и показать её прикладное значение. После прохождения вы сможете уверенно читать про градиентный спуск, loss-функции, ковариационные матрицы и байесовские методы — и понимать, о чём идёт речь.</p>
                    <div class="visual-guide">
                        <h4>Что разберём в модуле</h4>
                        <ul>
                            <li><strong>Линейная алгебра</strong> — векторы, матрицы, скалярное произведение; как данные хранятся и обрабатываются в коде и почему это связано с Attention в трансформерах.</li>
                            <li><strong>Производные и градиент</strong> — как модель «учится», двигая веса в направлении уменьшения ошибки.</li>
                            <li><strong>Теория вероятностей</strong> — вероятность, условная вероятность, формула Байеса; зачем это в классификаторах и генеративных моделях.</li>
                            <li><strong>Основы статистики</strong> — среднее, дисперсия, ковариация, корреляция; как этим пользоваться при разведке данных и выборе признаков.</li>
                        </ul>
                    </div>
                </div>
            </section>
            <section id="linear-algebra" class="topic-section">
                <h2>1.1 Линейная алгебра: не для абстракции, а для данных</h2>
                <div class="lesson-content">
                    <p>В машинном обучении данные почти всегда представлены числами: объект описывается набором признаков (возраст, доход, количество кликов и т.д.). Такой набор — это <strong>вектор</strong>. Линейная алгебра даёт язык и инструменты, чтобы работать с такими наборами компактно и быстро — и в коде (NumPy, PyTorch), и в формулах.</p>

                    <h3>Векторы и векторизация</h3>
                    <p><strong>Вектор</strong> — упорядоченный набор чисел. В ML одна строка таблицы (один объект, одно наблюдение) часто хранится как вектор: например, <code>[25, 50000, 12]</code> — возраст, зарплата, стаж. Размерность вектора — сколько в нём чисел; она совпадает с числом признаков.</p>
                    <p><strong>Векторизация</strong> — когда мы выполняем одну и ту же операцию сразу над целым массивом (вектором или матрицей), а не перебираем элементы в цикле. Так работают библиотеки NumPy, PyTorch, TensorFlow и видеокарты (GPU): одна инструкция обрабатывает много чисел параллельно. Поэтому векторизованный код на порядки быстрее циклов по элементам — это критично при обучении на больших данных.</p>

                    <h3>Матрицы как способ организации признаков</h3>
                    <p><strong>Матрица</strong> — это таблица чисел. В ML принято так: <strong>каждая строка</strong> — один объект (одно наблюдение), <strong>каждый столбец</strong> — один признак. То есть матрица размера 1000×10 — это 1000 объектов по 10 признаков. Одна строка такой матрицы — вектор признаков одного объекта.</p>
                    <p>Умножение матрицы на вектор (или матрицы на матрицу) даёт <strong>пакетную обработку</strong>: все объекты проходят через одни и те же веса. Один проход матричного умножения — это по сути применение одной «слойной» операции ко всем объектам выборки сразу. Так устроены и линейные модели, и слои нейросетей.</p>

                    <h3>Скалярное произведение и мера «похожести»</h3>
                    <p><strong>Скалярное произведение</strong> двух векторов одной длины — сумма произведений соответствующих элементов. Чем больше векторы «смотрят в одну сторону» (совпадают по знакам и пропорциям), тем больше скалярное произведение. Поэтому его часто используют как меру сходства.</p>
                    <p>Если векторы предварительно нормировать (сделать длину 1), то скалярное произведение превращается в <strong>косинусное сходство</strong> — число от −1 до 1, не зависящее от длины векторов. Именно на этом основан механизм <strong>Attention</strong> в трансформерах: запрос (Query) сравнивается с ключами (Key) через скалярное произведение (часто после масштабирования), чтобы решить, на какие части входа «смотреть» сильнее. Понимание векторов и скалярного произведения помогает читать описание Attention без ощущения «формулы с неба».</p>
                    <div class="note">
                        <p><strong>На практике:</strong> при разведке данных и в коде вы будете постоянно иметь дело с массивами и матрицами. Умение мысленно представлять «строка = объект, столбец = признак» и «умножение матрицы на веса = предсказание для всех объектов» сильно упрощает чтение кода и статей.</p>
                    </div>
                </div>
            </section>
            <section id="derivatives-gradient" class="topic-section">
                <h2>1.2 Производные и градиент</h2>
                <div class="lesson-content">
                    <p>Обучение модели в ML — это по сути поиск таких параметров (весов), при которых функция ошибки (loss) становится как можно меньше. Чтобы понять, <strong>в какую сторону</strong> двигать веса, нужны производные и градиент. Эта тема — сердце градиентного спуска и backpropagation.</p>

                    <h3>Физический смысл производной</h3>
                    <p><strong>Производная</strong> функции по аргументу показывает <strong>скорость изменения</strong> функции при малом изменении аргумента. Если производная положительная — функция растёт при увеличении аргумента; если отрицательная — убывает.</p>
                    <p>В ML аргументы — это веса модели, а функция — ошибка на данных (loss). <strong>Производная loss по весу</strong> отвечает на вопрос: если чуть-чуть увеличить этот вес, ошибка станет больше или меньше? Производная по весу указывает направление, в котором loss <strong>растёт</strong>. Нам нужно loss уменьшать, поэтому мы сдвигаем вес в <strong>противоположную</strong> сторону — в направлении убывания.</p>

                    <h3>Градиент и антиградиент</h3>
                    <p><strong>Градиент</strong> — это вектор, составленный из частных производных по всем параметрам. Он указывает направление <strong>наискорейшего подъёма</strong> функции: если сделать маленький шаг из текущей точки в направлении градиента, функция вырастет сильнее всего.</p>
                    <p>Нам нужно не поднимать, а <strong>опускать</strong> loss. Поэтому мы делаем шаг в направлении <strong>антиградиента</strong> (минус градиент): так мы двигаемся в сторону наискорейшего <strong>уменьшения</strong> ошибки. На каждом шаге обучения алгоритм (например, SGD) вычисляет градиент loss по весам и немного сдвигает веса в сторону антиградиента. Так модель «учится»: шаг за шагом веса подстраиваются под данные.</p>
                    <div class="visual-guide">
                        <h4>Кратко</h4>
                        <ul>
                            <li>Производная по весу = «насколько изменится loss при малом изменении этого веса».</li>
                            <li>Градиент = вектор всех таких производных; направление наискорейшего роста loss.</li>
                            <li>Антиградиент = направление наискорейшего уменьшения loss; в него и двигаем веса.</li>
                        </ul>
                    </div>
                </div>
            </section>
            <section id="probability" class="topic-section">
                <h2>1.3 Теория вероятностей и основные распределения</h2>
                <div class="lesson-content">
                    <p>В ML мы постоянно имеем дело с неопределённостью: данные содержат шум, предсказания модели — вероятности классов, а не жёсткие ответы. Теория вероятностей даёт язык для работы с такой неопределённостью и лежит в основе многих алгоритмов — от наивного Байеса до генеративных моделей.</p>

                    <h3>Вероятность как частота события</h3>
                    <p><strong>Вероятность</strong> события можно понимать как долю случаев, в которых это событие произойдёт при многократном повторении опыта. В ML мы часто не знаем «истинную» вероятность и <strong>оцениваем</strong> её по данным: например, доля объектов класса «спам» в размеченной выборке — это оценка вероятности класса. Такие оценки называются эмпирическими частотами и используются при обучении и оценке моделей.</p>

                    <h3>Условная вероятность и формула Байеса</h3>
                    <p><strong>Условная вероятность</strong> P(B|A) — вероятность события B при условии, что событие A уже произошло. Она лежит в основе многих рассуждений: «какова вероятность болезни при положительном тесте», «какова вероятность класса при данных признаках».</p>
                    <p><strong>Формула Байеса</strong> связывает условные вероятности P(A|B) и P(B|A) и позволяет «переворачивать» условие: обновлять вероятности гипотез при появлении новых данных. <strong>Наивный байесовский классификатор</strong> строит предсказание именно на формуле Байеса, предполагая условную независимость признаков при данном классе. В генеративных моделях мы явно моделируем распределения данных и условные вероятности; понимание P(B|A) и Байеса там необходимо.</p>

                    <h3>Нормальное распределение (гауссово)</h3>
                    <p><strong>Нормальное распределение</strong> (колокол Гаусса) — одно из самых важных в статистике и ML. Оно естественно возникает при суммировании многих независимых случайных величин (центральная предельная теорема): среднее по выборке, сумма ошибок и т.д. часто ведут себя «почти нормально».</p>
                    <p>Многие классические модели неявно опираются на нормальность: например, линейная регрессия с MSE оптимальна в предположении нормального распределения ошибок. Байесовские методы часто задают априорные распределения параметров как нормальные. Умение распознать «колокол» в формулах и допущениях помогает читать статьи и настраивать модели осознанно.</p>
                    <div class="practice-tips">
                        <h4>Где встретите в курсе</h4>
                        <p>Вероятности классов — в логистической регрессии и классификаторах; формула Байеса — в наивном Байесе; нормальное распределение — в обсуждении loss-функций и в байесовских подходах.</p>
                    </div>
                </div>
            </section>
            <section id="statistics" class="topic-section">
                <h2>1.4 Основы статистики</h2>
                <div class="lesson-content">
                    <p>Перед обучением модели данные нужно изучить: какие признаки есть, как они распределены, связаны ли друг с другом. Для этого используются простые статистические меры. Они же встречаются внутри алгоритмов (нормализация, отбор признаков, анализ важности) и в отчётах по качеству моделей.</p>

                    <h3>Среднее значение и дисперсия</h3>
                    <p><strong>Среднее значение</strong> (среднее арифметическое) — сумма всех значений признака, делённая на их количество. Это «центр масс» данных: типичное значение, вокруг которого разбросаны наблюдения.</p>
                    <p><strong>Дисперсия</strong> — средний квадрат отклонения значений от среднего. Она показывает <strong>разброс</strong>: если дисперсия большая, данные сильнее «размазаны»; если малая — лежат ближе к среднему. <strong>Стандартное отклонение</strong> — корень из дисперсии; оно измеряется в тех же единицах, что и сами данные (рубли, годы и т.д.), поэтому им удобно описывать разброс на практике.</p>

                    <h3>Ковариация и корреляция</h3>
                    <p><strong>Ковариация</strong> двух признаков — мера их совместной изменчивости. Если при увеличении одного второй в среднем тоже растёт, ковариация положительная; если один растёт, другой падает — отрицательная. Нулевая ковариация означает отсутствие линейной связи (но не обязательно независимость).</p>
                    <p><strong>Корреляция</strong> (например, коэффициент Пирсона) — это ковариация, нормированная так, чтобы значение лежало в отрезке от −1 до 1. По корреляции удобно судить о силе линейной связи: близко к 1 — сильная прямая связь, близко к −1 — сильная обратная, около 0 — слабая или нелинейная. Корреляцию можно считать и смотреть на тепловые карты (correlation heatmap) при разведке данных, не строя модель. Важно помнить: <strong>корреляция не означает причинно-следственную связь</strong> — два признака могут быть связаны из-за третьего фактора или случайности.</p>
                    <div class="note">
                        <p><strong>На практике:</strong> среднее и стандартное отклонение используются для нормализации признаков (привести к нулевому среднему и единичной дисперсии); корреляция — для отбора признаков и поиска мультиколлинеарности. Эти понятия будут встречаться в каждом реальном ML-проекте.</p>
                    </div>
                </div>
            </section>
        </main>
    </div>
    <div class="mobile-settings-menu" id="mobileSettingsMenu"><div class="mobile-settings-content"><div class="mobile-settings-header"><h3>Настройки</h3><button class="mobile-settings-close" id="mobileSettingsClose" aria-label="Закрыть"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path fill-rule="evenodd" d="M5.47 5.47a.75.75 0 011.06 0L12 10.94l5.47-5.47a.75.75 0 111.06 1.06L13.06 12l5.47 5.47a.75.75 0 11-1.06 1.06L12 13.06l-5.47 5.47a.75.75 0 01-1.06-1.06L10.94 12 5.47 6.53a.75.75 0 010-1.06z" clip-rule="evenodd"/></svg></button></div><div class="mobile-settings-section"><h4>Тема</h4><button class="mobile-theme-toggle-btn" id="mobileThemeToggle"><span class="theme-label">Светлая</span><svg class="theme-icon-sun" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path d="M12 2.25a.75.75 0 01.75.75v2.25a.75.75 0 01-1.5 0V3a.75.75 0 01.75-.75z"/></svg><svg class="theme-icon-moon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path fill-rule="evenodd" d="M9.528 1.718a.75.75 0 01.162.819A8.97 8.97 0 009 6a9 9 0 009 9 8.97 8.97 0 003.463-.69.75.75 0 01.981.98 10.503 10.503 0 01-9.694 6.46c-5.799 0-10.5-4.701-10.5-10.5 0-4.368 2.667-8.112 6.46-9.694a.75.75 0 01.818.162z" clip-rule="evenodd"/></svg></button></div></div></div>
    <script src="../courses.js"></script>
</body>
</html>
