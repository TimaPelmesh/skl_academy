<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Модуль 1: Математика и логика для Data Scientist — линейная алгебра, производные, градиент, вероятность, статистика">
    <title>1. Фундамент: математика и логика | Введение в AI | SKL Academy</title>
    <link rel="canonical" href="https://skl-academy.ru/courses/ai/01-math-logic.html">
    <link rel="stylesheet" href="../courses.css">
    <link rel="icon" href="../../images/icon.ico" type="image/x-icon">
</head>
<body data-course="ai">
    <header>
        <div class="menu-toggle" id="menuToggle"><span></span><span></span><span></span></div>
        <h1 class="header-title">Введение в AI</h1>
        <div class="header-buttons">
            <div class="color-scheme-buttons"><button class="color-scheme-btn" data-color="purple"><span class="color-dot" style="background: #6c63ff;"></span></button></div>
            <button class="theme-toggle-btn" id="themeToggle" title="Переключить тему" aria-label="Тема"><svg class="theme-icon-sun" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path d="M12 2.25a.75.75 0 01.75.75v2.25a.75.75 0 01-1.5 0V3a.75.75 0 01.75-.75zM7.5 12a4.5 4.5 0 119 0 4.5 4.5 0 01-9 0z"/></svg><svg class="theme-icon-moon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path fill-rule="evenodd" d="M9.528 1.718a.75.75 0 01.162.819A8.97 8.97 0 009 6a9 9 0 009 9 8.97 8.97 0 003.463-.69.75.75 0 01.981.98 10.503 10.503 0 01-9.694 6.46c-5.799 0-10.5-4.701-10.5-10.5 0-4.368 2.667-8.112 6.46-9.694a.75.75 0 01.818.162z" clip-rule="evenodd"/></svg></button>
            <button class="mobile-settings-btn" id="mobileSettingsTrigger" aria-label="Настройки"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M12.22 2h-.44a2 2 0 0 0-2 2v.18a2 2 0 0 1-1 1.73l-.43.25a2 2 0 0 1-2 0l-.15-.08a2 2 0 0 0-2.73.73l-.22.38a2 2 0 0 0 .73 2.73l.15.1a2 2 0 0 1 1 1.72v.51a2 2 0 0 1-1 1.74l-.15.09a2 2 0 0 0-.73 2.73l.22.38a2 2 0 0 0 2.73.73l.15-.08a2 2 0 0 1 2 0l.43.25a2 2 0 0 1 1 1.73V20a2 2 0 0 0 2 2h.44a2 2 0 0 0 2-2v-.18a2 2 0 0 1 1-1.73l.43-.25a2 2 0 0 1 2 0l.15.08a2 2 0 0 0 2.73-.73l.22-.39a2 2 0 0 0-.73-2.73l-.15-.08a2 2 0 0 1-1-1.74v-.5a2 2 0 0 1 1-1.74l.15-.09a2 2 0 0 0 .73-2.73l-.22-.38a2 2 0 0 0-2.73-.73l-.15.08a2 2 0 0 1-2 0l-.43-.25a2 2 0 0 1-1-1.73V4a2 2 0 0 0-2-2z"/><circle cx="12" cy="12" r="3"/></svg></button>
            <a href="../../index.html" class="exit-btn" title="На главную"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path d="M17 7l-1.41 1.41L18.17 11H8v2h10.17l-2.58 2.58L17 17l5-5zM4 5h8V3H4c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h8v-2H4V5z"/></svg></a>
        </div>
    </header>
    <div class="container">
        <div id="sidebar" class="sidebar">
            <div class="course-structure">
                <div class="topic"><button class="topic-btn active">1. Фундамент: математика и логика</button><div class="subtopics" style="max-height: 1000px;"><a href="#linear-algebra" class="subtopic active">1.1 Линейная алгебра для данных</a><a href="#derivatives-gradient" class="subtopic">1.2 Производные и градиент</a><a href="#probability" class="subtopic">1.3 Теория вероятностей</a><a href="#statistics" class="subtopic">1.4 Основы статистики</a></div></div>
                <div class="topic"><button class="topic-btn">2. Введение в ML</button><div class="subtopics"><a href="02-ml-intro.html#supervised" class="subtopic">2.1 С учителем</a><a href="02-ml-intro.html#unsupervised" class="subtopic">2.2 Без учителя</a><a href="02-ml-intro.html#pipeline" class="subtopic">2.4 Пайплайн</a></div></div>
                <div class="topic"><button class="topic-btn">3. Классические алгоритмы</button><div class="subtopics"><a href="03-classical-ml.html#linear-regression" class="subtopic">3.1 Линейная регрессия</a><a href="03-classical-ml.html#decision-trees" class="subtopic">3.4 Деревья</a></div></div>
                <div class="topic"><button class="topic-btn">4. Ансамблевые методы</button><div class="subtopics"><a href="04-ensembles.html#random-forest" class="subtopic">4.1 Random Forest</a><a href="04-ensembles.html#catboost" class="subtopic">4.3 CatBoost</a><a href="04-ensembles.html#lightgbm" class="subtopic">4.4 LightGBM</a></div></div>
                <div class="topic"><button class="topic-btn">5. Нейросети: введение</button><div class="subtopics"><a href="05-neural-intro.html#perceptron" class="subtopic">5.1 Перцептрон</a><a href="05-neural-intro.html#mlp" class="subtopic">5.3 MLP</a></div></div>
                <div class="topic"><button class="topic-btn">6. Backprop и оптимизация</button><div class="subtopics"><a href="06-backprop-optimization.html#backprop" class="subtopic">6.2 Backpropagation</a><a href="06-backprop-optimization.html#gradient-descent" class="subtopic">6.3 Градиентный спуск</a></div></div>
                <div class="topic"><button class="topic-btn">7. RNN и LSTM</button><div class="subtopics"><a href="07-rnn-lstm.html#rnn" class="subtopic">7.2 RNN</a><a href="07-rnn-lstm.html#lstm" class="subtopic">7.4 LSTM</a></div></div>
                <div class="topic"><button class="topic-btn">8. Attention и Трансформеры</button><div class="subtopics"><a href="08-attention-transformers.html#attention" class="subtopic">8.1 Attention</a><a href="08-attention-transformers.html#transformer" class="subtopic">8.2 Transformer</a></div></div>
                <div class="topic"><button class="topic-btn">9. LLM</button><div class="subtopics"><a href="09-llm.html#tokenization" class="subtopic">9.1 Токенизация</a><a href="09-llm.html#pretraining" class="subtopic">9.2 Pre-training</a></div></div>
                <div class="topic"><button class="topic-btn">10. Практикум и MLOps</button><div class="subtopics"><a href="10-practice-mlops.html#python-libs" class="subtopic">10.1 Библиотеки</a><a href="10-practice-mlops.html#mlops" class="subtopic">10.4 MLOps</a></div></div>
            </div>
        </div>
        <main id="content" class="content sidebar-active">
            <section id="module-intro" class="topic-section">
                <h2>Модуль 1: Фундамент. Математика и логика для Data Scientist'а</h2>
                <div class="lesson-content">
                    <p><strong>Задача модуля:</strong> Снять страх перед математикой и показать её прикладное значение.</p>
                </div>
            </section>
            <section id="linear-algebra" class="topic-section">
                <h2>Линейная алгебра не для абстракции, а для данных</h2>
                <div class="lesson-content">
                    <p><strong>Векторы и векторизация.</strong> Вектор — набор чисел (признаков объекта). Векторизация — когда операции выполняются сразу над целыми массивами, а не по элементам в цикле. Так работают NumPy и GPU: одна инструкция обрабатывает много данных, поэтому это сильно быстрее.</p>
                    <p><strong>Матрицы как способ организации признаков.</strong> Строки — объекты (наблюдения), столбцы — признаки. Одна строка = один вектор признаков. Умножение матрицы на вектор (или матрицы на матрицу) даёт пакетную обработку: все объекты проходят через одни и те же веса.</p>
                    <p><strong>Скалярное произведение как мера «похожести».</strong> Скалярное произведение двух векторов тем больше, чем они «больше в одну сторону». Нормализованное скалярное произведение (косинусное сходство) — основа механизма <strong>Attention</strong> в трансформерах: запрос (Query) сравнивается с ключами (Key) через скалярное произведение, чтобы понять, на какие части входа «смотреть».</p>
                </div>
            </section>
            <section id="derivatives-gradient" class="topic-section">
                <h2>Производные и градиент</h2>
                <div class="lesson-content">
                    <p><strong>Физический смысл производной</strong> — скорость изменения функции при изменении аргумента. В ML: насколько изменится ошибка (loss), если чуть сдвинуть вес. Производная по весу показывает направление, в котором loss растёт.</p>
                    <p><strong>Градиент</strong> — вектор из частных производных по всем параметрам. Он указывает в направлении <strong>наискорейшего подъёма</strong> функции. Нам нужно уменьшать loss, поэтому мы делаем шаг в направлении <strong>антиградиента</strong> (против градиента): так мы двигаемся в сторону скорейшего уменьшения ошибки. Так и «учится» модель: на каждом шаге смотрим градиент и немного сдвигаем веса в сторону антиградиента.</p>
                </div>
            </section>
            <section id="probability" class="topic-section">
                <h2>Теория вероятностей и основные распределения</h2>
                <div class="lesson-content">
                    <p><strong>Вероятность как частота события.</strong> При многократном повторении опыта доля исходов с нужным событием стремится к вероятности. В ML мы часто оцениваем вероятности по данным (эмпирические частоты).</p>
                    <p><strong>Условная вероятность</strong> P(B|A) — вероятность события B при условии, что произошло A. Формула Байеса и наивный байесовский классификатор строятся на этом. В генеративных моделях мы явно моделируем распределения и условные вероятности.</p>
                    <p><strong>Нормальное распределение</strong> (гауссово) часто возникает как предельное при суммировании многих случайных величин (центральная предельная теорема). Многие модели (например, линейная регрессия с MSE) неявно предполагают нормальность ошибок. Понимание «колокола» помогает читать про допущения моделей и байесовские методы.</p>
                </div>
            </section>
            <section id="statistics" class="topic-section">
                <h2>Основы статистики</h2>
                <div class="lesson-content">
                    <p><strong>Среднее значение</strong> — центр масс данных. <strong>Дисперсия</strong> — средний квадрат отклонения от среднего; показывает разброс. Корень из дисперсии (стандартное отклонение) — в тех же единицах, что и данные.</p>
                    <p><strong>Ковариация</strong> — мера совместной изменчивости двух признаков: положительная, если оба чаще растут вместе, отрицательная — если один растёт, другой падает. <strong>Корреляция</strong> (например, Пирсона) — нормированная ковариация, число от −1 до 1. По ней можно понять, связаны ли два признака линейно, не строя модель. Важно: корреляция не означает причинность.</p>
                </div>
            </section>
        </main>
    </div>
    <div class="mobile-settings-menu" id="mobileSettingsMenu"><div class="mobile-settings-content"><div class="mobile-settings-header"><h3>Настройки</h3><button class="mobile-settings-close" id="mobileSettingsClose" aria-label="Закрыть"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path fill-rule="evenodd" d="M5.47 5.47a.75.75 0 011.06 0L12 10.94l5.47-5.47a.75.75 0 111.06 1.06L13.06 12l5.47 5.47a.75.75 0 11-1.06 1.06L12 13.06l-5.47 5.47a.75.75 0 01-1.06-1.06L10.94 12 5.47 6.53a.75.75 0 010-1.06z" clip-rule="evenodd"/></svg></button></div><div class="mobile-settings-section"><h4>Тема</h4><button class="mobile-theme-toggle-btn" id="mobileThemeToggle"><span class="theme-label">Светлая</span><svg class="theme-icon-sun" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path d="M12 2.25a.75.75 0 01.75.75v2.25a.75.75 0 01-1.5 0V3a.75.75 0 01.75-.75z"/></svg><svg class="theme-icon-moon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path fill-rule="evenodd" d="M9.528 1.718a.75.75 0 01.162.819A8.97 8.97 0 009 6a9 9 0 009 9 8.97 8.97 0 003.463-.69.75.75 0 01.981.98 10.503 10.503 0 01-9.694 6.46c-5.799 0-10.5-4.701-10.5-10.5 0-4.368 2.667-8.112 6.46-9.694a.75.75 0 01.818.162z" clip-rule="evenodd"/></svg></button></div></div></div>
    <script src="../courses.js"></script>
</body>
</html>
