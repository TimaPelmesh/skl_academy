<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Модуль 4: Ансамблевые методы — Random Forest, градиентный бустинг, CatBoost, LightGBM">
    <title>4. Ансамблевые методы | Введение в AI | SKL Academy</title>
    <link rel="canonical" href="https://skl-academy.ru/courses/ai/04-ensembles.html">
    <link rel="stylesheet" href="../courses.css">
    <link rel="icon" href="../../images/icon.ico" type="image/x-icon">
</head>
<body data-course="ai">
    <header>
        <div class="menu-toggle" id="menuToggle"><span></span><span></span><span></span></div>
        <h1 class="header-title">Введение в AI</h1>
        <div class="header-buttons">
            <div class="color-scheme-buttons"><button class="color-scheme-btn" data-color="purple"><span class="color-dot" style="background: #6c63ff;"></span></button></div>
            <button class="theme-toggle-btn" id="themeToggle" aria-label="Тема"><svg class="theme-icon-sun" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path d="M12 2.25a.75.75 0 01.75.75v2.25a.75.75 0 01-1.5 0V3a.75.75 0 01.75-.75z"/></svg><svg class="theme-icon-moon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path fill-rule="evenodd" d="M9.528 1.718a.75.75 0 01.162.819A8.97 8.97 0 009 6a9 9 0 009 9 8.97 8.97 0 003.463-.69.75.75 0 01.981.98 10.503 10.503 0 01-9.694 6.46c-5.799 0-10.5-4.701-10.5-10.5 0-4.368 2.667-8.112 6.46-9.694a.75.75 0 01.818.162z" clip-rule="evenodd"/></svg></button>
            <button class="mobile-settings-btn" id="mobileSettingsTrigger" aria-label="Настройки"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M12.22 2h-.44a2 2 0 0 0-2 2v.18a2 2 0 0 1-1 1.73l-.43.25a2 2 0 0 1-2 0l-.15-.08a2 2 0 0 0-2.73.73l-.22.38a2 2 0 0 0 .73 2.73l.15.1a2 2 0 0 1 1 1.72v.51a2 2 0 0 1-1 1.74l-.15.09a2 2 0 0 0-.73 2.73l.22.38a2 2 0 0 0 2.73.73l.15-.08a2 2 0 0 1 2 0l.43.25a2 2 0 0 1 1 1.73V20a2 2 0 0 0 2 2h.44a2 2 0 0 0 2-2v-.18a2 2 0 0 1 1-1.73l.43-.25a2 2 0 0 1 2 0l.15.08a2 2 0 0 0 2.73-.73l.22-.39a2 2 0 0 0-.73-2.73l-.15-.08a2 2 0 0 1-1-1.74v-.5a2 2 0 0 1 1-1.74l.15-.09a2 2 0 0 0 .73-2.73l-.22-.38a2 2 0 0 0-2.73-.73l-.15.08a2 2 0 0 1-2 0l-.43-.25a2 2 0 0 1-1-1.73V4a2 2 0 0 0-2-2z"/><circle cx="12" cy="12" r="3"/></svg></button>
            <a href="../../index.html" class="exit-btn" title="На главную"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path d="M17 7l-1.41 1.41L18.17 11H8v2h10.17l-2.58 2.58L17 17l5-5zM4 5h8V3H4c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h8v-2H4V5z"/></svg></a>
        </div>
    </header>
    <div class="container">
        <div id="sidebar" class="sidebar">
            <div class="course-structure">
                <div class="topic">
                    <button class="topic-btn">1. Фундамент: математика и логика</button>
                    <div class="subtopics">
                        <a href="01-math-logic.html#linear-algebra" class="subtopic">1.1 Линейная алгебра для данных</a>
                        <a href="01-math-logic.html#derivatives-gradient" class="subtopic">1.2 Производные и градиент</a>
                        <a href="01-math-logic.html#probability" class="subtopic">1.3 Теория вероятностей</a>
                        <a href="01-math-logic.html#statistics" class="subtopic">1.4 Основы статистики</a>
                    </div>
                </div>
                <div class="topic">
                    <button class="topic-btn">2. Введение в ML и типы задач</button>
                    <div class="subtopics">
                        <a href="02-ml-intro.html#supervised" class="subtopic">2.1 Обучение с учителем</a>
                        <a href="02-ml-intro.html#unsupervised" class="subtopic">2.2 Обучение без учителя</a>
                        <a href="02-ml-intro.html#rl" class="subtopic">2.3 Обучение с подкреплением</a>
                        <a href="02-ml-intro.html#pipeline" class="subtopic">2.4 Пайплайн ML</a>
                    </div>
                </div>
                <div class="topic">
                    <button class="topic-btn">3. Классические алгоритмы</button>
                    <div class="subtopics">
                        <a href="03-classical-ml.html#linear-regression" class="subtopic">3.1 Линейная регрессия</a>
                        <a href="03-classical-ml.html#logistic" class="subtopic">3.2 Логистическая регрессия</a>
                        <a href="03-classical-ml.html#knn" class="subtopic">3.3 KNN</a>
                        <a href="03-classical-ml.html#decision-trees" class="subtopic">3.4 Решающие деревья</a>
                    </div>
                </div>
                <div class="topic">
                    <button class="topic-btn active">4. Ансамблевые методы</button>
                    <div class="subtopics" style="max-height: 1000px;">
                        <a href="#random-forest" class="subtopic active">4.1 Random Forest</a>
                        <a href="#gradient-boosting" class="subtopic">4.2 Градиентный бустинг</a>
                        <a href="#catboost" class="subtopic">4.3 CatBoost</a>
                        <a href="#lightgbm" class="subtopic">4.4 LightGBM</a>
                    </div>
                </div>
                <div class="topic">
                    <button class="topic-btn">5. Введение в нейросети</button>
                    <div class="subtopics">
                        <a href="05-neural-intro.html#perceptron" class="subtopic">5.1 Перцептрон</a>
                        <a href="05-neural-intro.html#activations" class="subtopic">5.2 Функции активации</a>
                        <a href="05-neural-intro.html#mlp" class="subtopic">5.3 MLP</a>
                        <a href="05-neural-intro.html#forward" class="subtopic">5.4 Forward Pass</a>
                        <a href="05-neural-intro.html#code-example" class="subtopic">5.5 Код на Python</a>
                        <a href="05-neural-intro.html#practice" class="subtopic">5.6 Практика</a>
                    </div>
                </div>
                <div class="topic">
                    <button class="topic-btn">6. Backpropagation и оптимизация</button>
                    <div class="subtopics">
                        <a href="06-backprop-optimization.html#loss" class="subtopic">6.1 Функция потерь</a>
                        <a href="06-backprop-optimization.html#backprop" class="subtopic">6.2 Backpropagation</a>
                        <a href="06-backprop-optimization.html#gradient-descent" class="subtopic">6.3 Градиентный спуск</a>
                        <a href="06-backprop-optimization.html#problems" class="subtopic">6.4 Проблемы обучения</a>
                    </div>
                </div>
                <div class="topic">
                    <button class="topic-btn">7. RNN и LSTM</button>
                    <div class="subtopics">
                        <a href="07-rnn-lstm.html#sequences" class="subtopic">7.1 Последовательные данные</a>
                        <a href="07-rnn-lstm.html#rnn" class="subtopic">7.2 RNN</a>
                        <a href="07-rnn-lstm.html#long-memory" class="subtopic">7.3 Долгосрочная память</a>
                        <a href="07-rnn-lstm.html#lstm" class="subtopic">7.4 LSTM</a>
                    </div>
                </div>
                <div class="topic">
                    <button class="topic-btn">8. Attention и Трансформеры</button>
                    <div class="subtopics">
                        <a href="08-attention-transformers.html#attention" class="subtopic">8.1 Механизм Attention</a>
                        <a href="08-attention-transformers.html#transformer" class="subtopic">8.2 Архитектура Transformer</a>
                        <a href="08-attention-transformers.html#positional" class="subtopic">8.3 Позиционное кодирование</a>
                        <a href="08-attention-transformers.html#multihead" class="subtopic">8.4 Multi-Head Attention</a>
                    </div>
                </div>
                <div class="topic">
                    <button class="topic-btn">9. Большие языковые модели (LLM)</button>
                    <div class="subtopics">
                        <a href="09-llm.html#tokenization" class="subtopic">9.1 Токенизация и эмбеддинги</a>
                        <a href="09-llm.html#pretraining" class="subtopic">9.2 Pre-training и Fine-tuning</a>
                        <a href="09-llm.html#temperature" class="subtopic">9.3 Температура и сэмплирование</a>
                        <a href="09-llm.html#context" class="subtopic">9.4 Окно контекста</a>
                    </div>
                </div>
                <div class="topic">
                    <button class="topic-btn">10. Практикум и MLOps</button>
                    <div class="subtopics">
                        <a href="10-practice-mlops.html#python-libs" class="subtopic">10.1 Библиотеки Python</a>
                        <a href="10-practice-mlops.html#sklearn" class="subtopic">10.2 Scikit-learn</a>
                        <a href="10-practice-mlops.html#pytorch-tf" class="subtopic">10.3 PyTorch / TensorFlow</a>
                        <a href="10-practice-mlops.html#mlops" class="subtopic">10.4 MLOps</a>
                    </div>
                </div>
            </div>
        </div>
        <main id="content" class="content sidebar-active">
            <section id="module-intro" class="topic-section">
                <h2>Модуль 4: Ансамблевые методы. Короли табличных данных</h2>
                <div class="lesson-content">
                    <p>Один алгоритм — даже мощное решающее дерево — может переобучиться или оказаться неустойчивым к шуму в данных. Идея ансамблей проста: объединить множество моделей так, чтобы их совместное решение было стабильнее и точнее, чем у каждой по отдельности. На табличных данных (признаки в столбцах, объекты в строках) ансамбли деревьев десятилетиями остаются рабочим стандартом в соревнованиях и в индустрии.</p>
                    <p><strong>Задача модуля:</strong> объяснить, как комбинация слабых моделей даёт «супер-силу», разобрать бэггинг и случайный лес, затем перейти к градиентному бустингу и его современным реализациям — CatBoost и LightGBM. После модуля вы будете понимать, почему в задачах на таблицах чаще всего выбирают именно эти методы и как они устроены под капотом.</p>
                    <div class="visual-guide">
                        <h4>Что разберём</h4>
                        <ul>
                            <li><strong>Random Forest</strong> — бутстрэп, бэггинг, случайность по признакам.</li>
                            <li><strong>Градиентный бустинг</strong> — последовательное усиление, остатки и градиент как цель.</li>
                            <li><strong>CatBoost</strong> — категориальные признаки, симметричные деревья, ordered boosting.</li>
                            <li><strong>LightGBM</strong> — GOSS, EFB, листовой рост; когда и зачем использовать.</li>
                        </ul>
                    </div>
                </div>
            </section>
            <section id="random-forest" class="topic-section">
                <h2>4.1 Бутстрэп и случайный лес (Random Forest)</h2>
                <div class="lesson-content">
                    <p>Одно дерево решений хорошо интерпретируется, но сильно зависит от конкретной выборки: небольшое изменение данных может изменить вид дерева и предсказания. Чтобы снизить эту дисперсию, придумали <strong>бэггинг</strong> (Bootstrap Aggregating): много раз с повторениями берём случайную подвыборку из обучающих данных, на каждой подвыборке обучаем своё дерево, а итоговый ответ получаем усреднением (регрессия) или голосованием (классификация). Так мы «сглаживаем» ошибки отдельных деревьев и получаем более стабильную модель.</p>
                    <p><strong>Random Forest</strong> развивает эту идею: к случайности по объектам добавляется случайность по признакам. При построении каждого разбиения в узле дерева рассматривается не весь набор признаков, а лишь случайное подмножество (например, корень из числа признаков). В результате деревья в ансамбле становятся более разнообразными: они по-разному смотрят на данные и по-разному ошибаются. При голосовании или усреднении такие ошибки частично компенсируются.</p>
                    <p>Итог — ансамбль из десятков или сотен «шумных», но независимых деревьев, который часто даёт отличное качество на табличных данных без тонкой настройки. Random Forest устойчив к выбросам, умеет работать с пропусками и не требует масштабирования признаков; его удобно использовать как сильный бейзлайн перед переходом к бустингу.</p>
                    <div class="note">
                        <p><strong>На практике:</strong> для регрессии ответ ансамбля — среднее предсказаний деревьев; для классификации — класс, набравший большинство голосов (или усреднение вероятностей, если деревья их выдают). Гиперпараметры: число деревьев, максимальная глубина, размер случайного подмножества признаков.</p>
                    </div>
                </div>
            </section>
            <section id="gradient-boosting" class="topic-section">
                <h2>4.2 Суть градиентного бустинга</h2>
                <div class="lesson-content">
                    <p>В бэггинге модели обучаются независимо и затем усредняются. В <strong>градиентном бустинге</strong> подход другой: модели добавляются последовательно, и каждая следующая учится исправлять ошибки предыдущих. Метафора — комитет экспертов, где каждый новый член фокусируется на тех примерах, где предыдущие ошиблись сильнее всего.</p>
                    <p>Математически на каждом шаге мы считаем <strong>остатки</strong> — разницу между истинной целью и текущим предсказанием ансамбля — или, в общем случае, <strong>градиент функции потерь</strong> по предсказаниям. Следующее дерево обучается предсказывать именно эти остатки (или антиградиент); его предсказание добавляется к ансамблю с небольшим коэффициентом (learning rate), чтобы не «перешагнуть» оптимум. Так шаг за шагом строится сильный ансамбль: каждое новое дерево вносит корректировку в направлении уменьшения ошибки.</p>
                    <p>Градиентный бустинг деревьев (Gradient Boosting Decision Trees, GBDT) лежит в основе XGBoost, CatBoost, LightGBM и многих побед на Kaggle. Понимание идеи «усиливать слабые модели, исправляя их ошибки» помогает осознанно настраивать число итераций, глубину деревьев и learning rate.</p>
                    <div class="visual-guide">
                        <h4>Кратко</h4>
                        <p>Бэггинг: много моделей параллельно, ответ — усреднение. Бустинг: много моделей последовательно, каждая исправляет ошибки предыдущей. В бустинге цель для следующего дерева — остатки или градиент loss.</p>
                    </div>
                </div>
            </section>
            <section id="catboost" class="topic-section">
                <h2>4.3 CatBoost: категории, симметрия, ordered boosting</h2>
                <div class="lesson-content">
                    <p>CatBoost — одна из ведущих реализаций градиентного бустинга, разработанная в Яндексе. Она особенно удобна, когда в данных много <strong>категориальных признаков</strong> (регион, тип товара, тег). Обычно такие признаки превращают в числа через One-Hot Encoding или Label Encoding, что раздувает размерность или теряет порядок. CatBoost умеет работать с категориями напрямую: использует <strong>ordered target encoding</strong> — кодирование по целевой переменной с перестановками объектов, чтобы избежать «утечки» информации из будущего и снизить переобучение.</p>
                    <p>Вторая отличительная черта — <strong>симметричные деревья</strong>: на одном уровне во всех узлах применяется одно и то же разбиение по одному и тому же признаку (но с разными порогами для левого и правого поддерева). Это ускоряет инференс и упрощает аппаратную оптимизацию.</p>
                    <p><strong>Ordered boosting</strong> решает проблему смещения градиента. В обычном бустинге при подсчёте градиента для объекта могут неявно использоваться данные этого же объекта через статистики по выборке. В CatBoost при вычислении градиента для объекта используются только «предыдущие» объекты (в смысле случайной перестановки), поэтому оценка градиента получается несмещённой, и переобучение снижается. В итоге CatBoost часто даёт отличное качество «из коробки» при минимуме настроек и хорошо подходит для задач с категориями и таблицами.</p>
                    <div class="note">
                        <p><strong>Когда выбирать CatBoost:</strong> много категориальных признаков, нужна быстрая сходимость и стабильное качество без долгой подстройки. Удобен в продакшене благодаря скорости предсказания и встроенной поддержке GPU.</p>
                    </div>
                </div>
            </section>
            <section id="lightgbm" class="topic-section">
                <h2>4.4 LightGBM: скорость и масштаб</h2>
                <div class="lesson-content">
                    <p>LightGBM (Light Gradient Boosting Machine) от Microsoft ориентирован на высокую скорость обучения и работу с большими объёмами данных. Достигается это за счёт нескольких идей. <strong>GOSS</strong> (Gradient-based One-Side Sampling) — односторонняя выборка по градиенту: объекты с большими градиентами (то есть те, на которых модель ошибается сильнее) сохраняются полностью, а среди остальных выполняется случайная подвыборка. Так мы экономим время на обучении, не теряя важных «сложных» примеров.</p>
                    <p><strong>EFB</strong> (Exclusive Feature Bundling) решает проблему разреженных признаков (много нулей): взаимно исключающие признаки объединяются в «бандлы», и при разбиении узла перебирается меньше суррогатных разбиений. Память и время счёта сокращаются без существенной потери качества.</p>
                    <p>Третья особенность — <strong>листовой (leaf-wise)</strong> рост дерева. Вместо того чтобы на каждом уровне разбивать все листья (level-wise, как в XGBoost по умолчанию), LightGBM на каждой итерации выбирает один лист с максимальным приростом качества и разбивает только его. В результате при том же числе листьев дерево получается глубже и часто точнее, хотя риск переобучения чуть выше — его ограничивают регуляризацией и глубиной.</p>
                    <p>LightGBM широко используется в соревнованиях и в индустрии, когда важны скорость обучения, экономия памяти и качество на больших таблицах. Выбор между CatBoost и LightGBM часто делают по результатам кросс-валидации и удобству интеграции в пайплайн.</p>
                    <div class="practice-tips">
                        <h4>Практический совет</h4>
                        <p>Для табличных данных без изображений и текста обычно пробуют Random Forest как бейзлайн, затем градиентный бустинг (CatBoost или LightGBM). Ансамбли деревьев остаются стандартом выбора во многих прикладных задачах прогнозирования и классификации.</p>
                    </div>
                </div>
            </section>
        </main>
    </div>
    <div class="mobile-settings-menu" id="mobileSettingsMenu"><div class="mobile-settings-content"><div class="mobile-settings-header"><h3>Настройки</h3><button class="mobile-settings-close" id="mobileSettingsClose" aria-label="Закрыть"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path fill-rule="evenodd" d="M5.47 5.47a.75.75 0 011.06 0L12 10.94l5.47-5.47a.75.75 0 111.06 1.06L13.06 12l5.47 5.47a.75.75 0 11-1.06 1.06L12 13.06l-5.47 5.47a.75.75 0 01-1.06-1.06L10.94 12 5.47 6.53a.75.75 0 010-1.06z" clip-rule="evenodd"/></svg></button></div><div class="mobile-settings-section"><h4>Тема</h4><button class="mobile-theme-toggle-btn" id="mobileThemeToggle"><span class="theme-label">Светлая</span><svg class="theme-icon-sun" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path d="M12 2.25a.75.75 0 01.75.75v2.25a.75.75 0 01-1.5 0V3a.75.75 0 01.75-.75z"/></svg><svg class="theme-icon-moon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path fill-rule="evenodd" d="M9.528 1.718a.75.75 0 01.162.819A8.97 8.97 0 009 6a9 9 0 009 9 8.97 8.97 0 003.463-.69.75.75 0 01.981.98 10.503 10.503 0 01-9.694 6.46c-5.799 0-10.5-4.701-10.5-10.5 0-4.368 2.667-8.112 6.46-9.694a.75.75 0 01.818.162z" clip-rule="evenodd"/></svg></button></div></div></div>
    <script src="../courses.js"></script>
</body>
</html>
